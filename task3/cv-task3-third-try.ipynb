{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd ../input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd cvdl-task2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from abbyy_course_cvdl_t2.impl.data import CocoTextDetection, CocoDetectionPrepareTransform\nfrom abbyy_course_cvdl_t2.convert import ObjectsToPoints\nfrom abbyy_course_cvdl_t2.loss import CenterNetLoss\nfrom abbyy_course_cvdl_t2.network import CenterNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls ../cv-task3-data/task3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ../cv-task3-data/task3/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport torch\nimport math\nimport warnings\nfrom tqdm import tqdm\nfrom abbyy_course_cvdl_t3.coco_text import COCO_Text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base = Path(\"D:\\\\data\\\\Coco\")\n# anno_path = base / 'cocotext.v2.json'\n# images_path = base / 'train2014'\n\nanno_path = Path(\"./abbyy_course_cvdl_t3/data/cocotext.v2.json\")\nimages_path = Path(\"../../coco-2014-dataset-for-yolov3/coco2014/images/train2014\")\n\nassert anno_path.exists(), \"Set your own path to annotation\"\nassert images_path.exists(), \"Set your own path to images\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"СOCO_Text взят из репозитория авторов датасета\nhttps://github.com/andreasveit/coco-text/","metadata":{}},{"cell_type":"code","source":"ct = COCO_Text(anno_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Пример работы из репозитория coco-text\nВзято из \nhttps://github.com/andreasveit/coco-text/blob/master/coco_text_Demo.ipynb","metadata":{}},{"cell_type":"code","source":"# get all images containing at least one instance of legible text\nimgIds = ct.getImgIds(imgIds=ct.train, \n                    catIds=[('legibility','legible')])\n# pick one at random\nimg = ct.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (10.0, 8.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread(images_path / img['file_name']))\nplt.axis(False)\nplt.title(\"Изображение из датасета COCO\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread(images_path / img['file_name']))\nannIds = ct.getAnnIds(imgIds=img['id'])\nanns = ct.loadAnns(annIds)\nct.showAnns(anns)\nplt.title(\"Изображение с GT детекциями текста\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Подготовка датасетов","metadata":{}},{"cell_type":"code","source":"!pip install editdistance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from abbyy_course_cvdl_t3.coco_text import COCO_Text\nfrom abbyy_course_cvdl_t3 import coco_evaluation\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train = CocoTextDetection(\n    images_path,\n    Path(anno_path),\n    transforms=CocoDetectionPrepareTransform(size=(640, 640))\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_val = CocoTextDetection(\n    images_path,\n    Path(anno_path),\n    transforms=CocoDetectionPrepareTransform(size=(640, 640)),\n    split='val'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Подготовка модели\n","metadata":{}},{"cell_type":"code","source":"obj_to_points = ObjectsToPoints(hw=160, num_classes=1, smooth_kernel_size=3)\nloss = CenterNetLoss(obj_to_points=obj_to_points)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = CenterNet(head_kwargs={'c_classes': 1}, nms_kwargs={'kernel_size': 3})\ncrit = CenterNetLoss(obj_to_points=obj_to_points)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu = torch.device('cuda:0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = net.to(gpu);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка","metadata":{}},{"cell_type":"code","source":"from abbyy_course_cvdl_t3.utils import dump_detections_to_cocotext_json\nfrom abbyy_course_cvdl_t3.utils import evaluate_ap_from_cocotext_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_dataset, val_dataset, *, net=None, criterion=None, \n          train_batch_size=1, val_batch_size = 1,  lr=3e-4, \n          epochs=1, image_size=(640, 640), device=None):\n    if net is None:\n        net = CenterNet(pretrained=True)\n    if criterion is None:\n        criterion = CenterNetLoss()\n\n    if device is not None:\n        net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n    trainloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=2\n    )\n    valloader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=2\n    )\n    stats_step = (len(train_dataset) // 500 // train_batch_size) + 1\n    for epoch in range(epochs):\n        if epoch == 0:\n            # на первой эпохе учимся с малым lr, чтобы не сломать pretrain\n            optimizer.lr = lr / 1000\n        else:\n            # дальше постепенно уменьшаем\n            optimizer.lr = lr / 2**epoch\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            inputs, anno = data\n            print('inputs.shape = ', inputs.shape)\n            inputs = inputs.to(device)\n            anno = anno.to(device)\n            optimizer.zero_grad()\n            \n            outputs = net(inputs)\n            print('outputs.shape = ', outputs.shape)\n            print('anno.shape = ', anno.shape)\n            losses = criterion(outputs, anno).mean(axis=0)\n            loss_value = losses.sum()\n            if torch.isnan(loss_value).any():\n                warnings.warn(\"nan loss! skip update\")\n                print(f\"last loss: {[l.item() for l in losses]}\")\n                print(inputs.cpu().numpy())\n                print(outputs[0].detach().cpu().numpy())\n                continue\n            running_loss += loss_value\n            if (i % stats_step == 0):\n                print(f\"epoch {epoch}|{i}; total loss:{running_loss / stats_step}\")\n                print(f\"last losses: {[l.item() for l in losses.flatten()]}\")\n                running_loss = 0.0\n            \n            loss_value.backward()\n            optimizer.step()\n            \n            net.eval()\n            prepared_preds = []\n            img_ids = []\n\n        with torch.no_grad():\n            for num, data in enumerate(valloader):\n                x, target = data\n                x = x.to(device)\n                target = target.to(device)\n                pred = net(x)\n                prepared_preds.append(\n                    pred,\n                    size_src=[img_meta['width'], img_meta['height']], \n                    size_current=[160, 160]\n                )\n                \n                scores = np.concatenate([u['scores'] for u in prepared_preds])\n                boxes = np.concatenate([u['boxes'] for u in prepared_preds], axis=0)\n\n            dump_detections_to_cocotext_json(\n                image_ids = image_ids.tolist(),\n                xlefts=boxes[:, 0].tolist(),\n                ytops=boxes[:, 1].tolist(),\n                widths=boxes[:, 2].tolist(),\n                heights=boxes[:, 3].tolist(),\n                scores=scores.tolist(),\n                path='../../../working/predictions.json'\n            )\n            ap, prec, rec = evaluate_ap_from_cocotext_json(\n                coco_text=ct,\n                path='../../../working/predictions.json'\n            )\n            \n            torch.save(net, '../../../working/centralnet_coco_text_third_try_epoch_' + str(i) + '.pth')\n            last_path = '../../../working/centralnet_coco_text_third_try_epoch_' + str(i) + '.pth'\n            print(f\"Итоговый скор AP на val: {ap}\")\n            print('saved')\n            net.train()\n            \n    print('Finished Training')\n    return net\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Запуск тренировки","metadata":{}},{"cell_type":"code","source":"net = train(ds_train, ds_val, net=net, criterion=crit, train_batch_size = 1, val_batch_size = 1, epochs=1, device=torch.device('cuda:0'), lr=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Сохраняем модель на диск","metadata":{}},{"cell_type":"code","source":"# %ls ../../..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(model, '../../../working/retina_coco_text_first_try_epoch_' + str(t) + '.pth')   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.eval();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"img_id = 101243\nimg_meta = ct.loadImgs(ids=[img_id])[0]\nplt.imshow(plt.imread(images_path / img_meta['file_name']))\nannIds = ct.getAnnIds(imgIds=img_meta['id'])\nanns = ct.loadAnns(annIds)\nct.showAnns(anns)\nplt.title(f\"GT: {img_meta['id']}\")\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img, anno = ds_val[ds_val.ids.index(str(img_id))]\n# preds = model([img.to(gpu)])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import torchvision\n\nplt.imshow(\n    torchvision.utils.draw_bounding_boxes(\n        (img * 255 ).type(torch.uint8), \n        preds['boxes'],\n    ).permute(1, 2, 0),\n)\nplt.title(\"Pred: все боксы\")\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"high_confidence_scores = preds['scores'] > 0.35\nhigh_confidence_boxes = preds['boxes'][high_confidence_scores]\nplt.imshow(\n    torchvision.utils.draw_bounding_boxes(\n        (img * 255 ).type(torch.uint8), \n        high_confidence_boxes\n    ).permute(1, 2, 0),\n)\nplt.title(\"Pred: боксы с score > 0.35\")\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"prepared_preds = []\nimg_ids = []\n\nfor num, img_id in enumerate(tqdm(ds_val.ids)):\n    img_id = int(img_id)\n    img_meta = ct.loadImgs(ids=[img_id])[0]\n    with torch.no_grad():\n        x = ds_val[num][0]\n        pred = model([\n            x.to(gpu)\n        ])[0]\n        prepared_preds.append(\n            postprocess(\n                pred,\n                size_src=[img_meta['width'], img_meta['height']], \n                size_current=[640, 640]\n            )\n        )\n        img_ids.append(img_id)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from abbyy_course_cvdl_t3.utils import dump_detections_to_cocotext_json\nscores = np.concatenate([u['scores'] for u in prepared_preds])\nboxes = np.concatenate([u['boxes'] for u in prepared_preds], axis=0)\nimage_ids = []\nfor num, i in enumerate(img_ids):\n    image_ids += [i] * len(prepared_preds[num]['boxes'])\nimage_ids = np.array(image_ids)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"dump_detections_to_cocotext_json(\n    image_ids = image_ids.tolist(),\n    xlefts=boxes[:, 0].tolist(),\n    ytops=boxes[:, 1].tolist(),\n    widths=boxes[:, 2].tolist(),\n    heights=boxes[:, 3].tolist(),\n    scores=scores.tolist(),\n    path='../../../working/predictions.json'\n);\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from abbyy_course_cvdl_t3.utils import evaluate_ap_from_cocotext_json\nap, prec, rec = evaluate_ap_from_cocotext_json(\n    coco_text=ct,\n    path='../../../working/predictions.json'\n)\nprint(f\"Итоговый скор AP на val: {ap}\")\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from matplotlib import pyplot as plt\nplt.plot(prec, rec)\nplt.xlabel('precision')\nplt.ylabel('recall')\nplt.title('PR curve')\nplt.grid()\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}